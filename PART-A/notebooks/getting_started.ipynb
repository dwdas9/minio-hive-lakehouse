{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541191b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session created!\n",
      "✓ Connected to Hive Metastore\n",
      "✓ Iceberg catalog ready\n",
      "\n",
      "Spark version: 3.4.1\n",
      "Default catalog: iceberg\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session - uses spark-defaults.conf, no manual config needed\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive-Iceberg-MinIO\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark session created!\")\n",
    "print(\"✓ Connected to Hive Metastore\")\n",
    "print(\"✓ Iceberg catalog ready\")\n",
    "print()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default catalog: {spark.catalog.currentCatalog()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5d31b",
   "metadata": {},
   "source": [
    "## Create a Database and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d857b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database 'demo' created!\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     demo|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database (namespace)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "print(\"✓ Database 'demo' created!\")\n",
    "\n",
    "# Show databases\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5bde3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Table 'demo.users' created!\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|     demo|    users|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS demo.users (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        created_at TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(created_at))\n",
    "\"\"\")\n",
    "print(\"✓ Table 'demo.users' created!\")\n",
    "\n",
    "# Show tables\n",
    "spark.sql(\"SHOW TABLES IN demo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69ad9a",
   "metadata": {},
   "source": [
    "## Insert and Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca05da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data inserted!\n"
     ]
    }
   ],
   "source": [
    "# Insert some data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.users VALUES\n",
    "    (1, 'Alice', 'alice@example.com', current_timestamp()),\n",
    "    (2, 'Bob', 'bob@example.com', current_timestamp()),\n",
    "    (3, 'Charlie', 'charlie@example.com', current_timestamp())\n",
    "\"\"\")\n",
    "print(\"✓ Data inserted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65631cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+--------------------+\n",
      "| id|   name|              email|          created_at|\n",
      "+---+-------+-------------------+--------------------+\n",
      "|  1|  Alice|  alice@example.com|2025-11-30 08:22:...|\n",
      "|  2|    Bob|    bob@example.com|2025-11-30 08:22:...|\n",
      "|  3|Charlie|charlie@example.com|2025-11-30 08:22:...|\n",
      "+---+-------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the data\n",
    "spark.sql(\"SELECT * FROM demo.users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ac408",
   "metadata": {},
   "source": [
    "## Updates and Deletes (Iceberg feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12603376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated Alice's email\n",
      "✓ Deleted Charlie\n",
      "+---+-----+--------------------+--------------------+\n",
      "| id| name|               email|          created_at|\n",
      "+---+-----+--------------------+--------------------+\n",
      "|  1|Alice|alice.new@example...|2025-11-30 08:22:...|\n",
      "|  2|  Bob|     bob@example.com|2025-11-30 08:22:...|\n",
      "+---+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update a row\n",
    "spark.sql(\"UPDATE demo.users SET email = 'alice.new@example.com' WHERE id = 1\")\n",
    "print(\"✓ Updated Alice's email\")\n",
    "\n",
    "# Delete a row\n",
    "spark.sql(\"DELETE FROM demo.users WHERE id = 3\")\n",
    "print(\"✓ Deleted Charlie\")\n",
    "\n",
    "# Check results\n",
    "spark.sql(\"SELECT * FROM demo.users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47337f47",
   "metadata": {},
   "source": [
    "## Time Travel (Iceberg feature)\n",
    "\n",
    "Every change creates a snapshot. You can query historical versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becc31e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-11-29 16:37:29.301|8156896692273332934|null               |true               |\n",
      "|2025-11-29 16:37:41.102|462503457610497467 |8156896692273332934|true               |\n",
      "|2025-11-29 16:37:41.534|155410955388836578 |462503457610497467 |true               |\n",
      "|2025-11-29 17:49:11.63 |6866363440748165282|155410955388836578 |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "spark.sql(\"SELECT * FROM demo.users.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fecfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|6264401885849743558|2025-11-30 08:22:46.497|append   |\n",
      "|6412877392095783392|2025-11-30 08:22:53.794|overwrite|\n",
      "|1922986721336350599|2025-11-30 08:22:54.46 |overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View snapshots\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM demo.users.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb35cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query an older snapshot (get snapshot_id from above)\n",
    "# Uncomment and replace with actual snapshot_id:\n",
    "# spark.sql(\"SELECT * FROM demo.users VERSION AS OF <snapshot_id>\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8904d",
   "metadata": {},
   "source": [
    "## Schema Evolution (Iceberg feature)\n",
    "\n",
    "Add, drop, rename columns without rewriting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2db9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added 'age' column\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|            id|             int|   null|\n",
      "|          name|          string|   null|\n",
      "|         email|          string|   null|\n",
      "|    created_at|       timestamp|   null|\n",
      "|           age|             int|   null|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|days(created_at)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column\n",
    "spark.sql(\"ALTER TABLE demo.users ADD COLUMN age INT\")\n",
    "print(\"✓ Added 'age' column\")\n",
    "\n",
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE demo.users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3657ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+--------------------+---+\n",
      "| id| name|               email|          created_at|age|\n",
      "+---+-----+--------------------+--------------------+---+\n",
      "|  1|Alice|alice.new@example...|2025-11-30 08:22:...| 30|\n",
      "|  2|  Bob|     bob@example.com|2025-11-30 08:22:...| 25|\n",
      "+---+-----+--------------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update with new column\n",
    "spark.sql(\"UPDATE demo.users SET age = 30 WHERE id = 1\")\n",
    "spark.sql(\"UPDATE demo.users SET age = 25 WHERE id = 2\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b75e91",
   "metadata": {},
   "source": [
    "## Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdff38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "|file_path                                                                                                               |file_size_in_bytes|record_count|\n",
      "+------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "|s3a://warehouse/demo.db/users/data/created_at_day=2025-11-30/00000-20-6bdf6a0f-969c-4506-9968-2ec81e2c314d-00001.parquet|1472              |2           |\n",
      "+------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View files that make up the table\n",
    "spark.sql(\"SELECT file_path, file_size_in_bytes, record_count FROM demo.users.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438d8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|   partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|{2025-11-30}|      0|           2|         1|                         1472|                           0|                         0|                           0|                         0|2025-11-30 08:23:...|     2191539070265404586|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View partitions\n",
    "spark.sql(\"SELECT * FROM demo.users.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637bdefc",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db625f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table (uncomment to run)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS demo.users\")\n",
    "# print(\"✓ Table dropped\")\n",
    "\n",
    "# Drop database (uncomment to run)\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS demo CASCADE\")\n",
    "# print(\"✓ Database dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726db073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (optional, frees resources)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
